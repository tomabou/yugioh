6/14
基本的なカードリストを実装した

アルゴリズムに対する考察
状態数が10^20~10^30くらいで、各状態で100〜1000くらいの遷移先があって(確率的に決まることもある)、だいたい100回くらい遷移した後に成功できるか判定できるみたいなタスク
環境のモデルが完全に既知なんだけど、状態数が多くて近似なしに動的計画法するのは無理みたいなシチュエーション

一番安直にやろうとすると成功した瞬間を報酬1、他は0で、減衰率γ=1にしてn-step DQNのようなものを回して勝率=Q(s,a)を求める
関数近似器はDNNじゃなくていい

報酬も完全に既知だから、状態とアクションに依存した関数Q(s,a)じゃなくて状態だけに依存したV(s)で諸々のアルゴリズムが書ける

    書ける気もするけど、結局行動選ぶときにQ(s,a)に類するものを計算することになる
    性能もあまり変わらないのではないか

Rainbowまで盛り盛りしたほうが強いっちゃ強い
Double DQNにnstep載せるまではmustな香りがする (報酬がかなりスパースなのでnstep分伝播しないと学習が遅いはず)

Prioritized Experience Replayはたぶん効く
Duelingは微妙かも、分布型(C51/QR-DQN)は載せられたら載せたい
この辺のパーツの自前実装は一部苦行なのでpfrlあたりのライブラリを適当に引っ張ってくると良さそう


モデルが完全に分かってることによるDQNの改善
    (s,a)の組が与えられたときの遷移・報酬がわかっているので、(s,a)の組に対する期待値計算が正確にできる
    ほかは不明

6/15
遊戯王のコードを実装する上で強化学習フレームワークの使い方を知っておきたい
とりあえずpfrlの環境構築をしてみる